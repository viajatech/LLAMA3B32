#pip install --upgrade transformers torch
#pip install transformers torch langdetect







import tkinter as tk
from tkinter import scrolledtext
from tkinter import ttk
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import threading

# Token de acceso personal de Hugging Face
HF_TOKEN = "PON AQUI TU TOKEN READ!!!!"

# Configurar dispositivo (GPU si está disponible)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Nombre del modelo
model_name = "meta-llama/Llama-3.2-3B"

def load_model():
    status_label.config(text="Cargando el modelo, por favor espera...")
    progress_bar.start()

    def model_loading():
        try:
            # Liberar caché de GPU antes de cargar el modelo
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # Cargar el tokenizador y el modelo
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                use_auth_token=HF_TOKEN,
                trust_remote_code=True
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                use_auth_token=HF_TOKEN,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                trust_remote_code=True
            )
            # Mover el modelo explícitamente a la GPU
            model.to(device)
            model.eval()  # Poner el modelo en modo evaluación

            status_label.config(text="Modelo cargado. ¡Ya puedes chatear!")
            send_button.config(state=tk.NORMAL)
            user_entry.config(state=tk.NORMAL)
            clear_button.config(state=tk.NORMAL)
            root.model = model
            root.tokenizer = tokenizer
        except Exception as e:
            status_label.config(text=f"Error al cargar el modelo: {e}")
        finally:
            progress_bar.stop()

    threading.Thread(target=model_loading).start()

def generate_response(event=None):
    user_input = user_entry.get()
    if not user_input.strip():
        return  # No enviar si la entrada está vacía
    user_entry.delete(0, tk.END)
    chat_log.config(state=tk.NORMAL)
    chat_log.insert(tk.END, f"{user_name.get()}: {user_input}\n")
    chat_log.config(state=tk.DISABLED)
    chat_log.see(tk.END)

    # Historial de la conversación
    if not hasattr(root, 'conversation_history'):
        root.conversation_history = []

    # Añadir la última interacción al historial con formato claro
    root.conversation_history.append(f"<{user_name.get()}>: {user_input}\n")

    # Limitar el historial a las últimas N líneas
    MAX_HISTORY_LINES = 10  # Puedes ajustar este valor
    if len(root.conversation_history) > MAX_HISTORY_LINES:
        root.conversation_history = root.conversation_history[-MAX_HISTORY_LINES:]

    def run_model():
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()  # Liberar caché de GPU antes de generar

            # Preparar el prompt y el input
            prompt = (
                f"Eres un asistente de inteligencia artificial llamado {bot_name.get()} que responde en español. "
                f"Debes seguir las instrucciones al pie de la letra, proporcionar respuestas lógicas y coherentes, "
                f"y mantenerte en el tema de conversación. Si alguien te llama por tu nombre, {bot_name.get()}, debes responder adecuadamente.\n"
                f"Formato de la conversación:\n"
                "<Usuario>: Hola, ¿cómo estás?\n"
                f"<{bot_name.get()}>: Estoy bien, gracias. ¿En qué puedo ayudarte?\n\n"
                "Comencemos la conversación:\n"
            )

            input_text = prompt + ''.join(root.conversation_history) + f"<{bot_name.get()}>:"

            # Codificar el texto de entrada
            input_ids = root.tokenizer.encode(input_text, return_tensors='pt')

            # Mover los tensores a la GPU
            input_ids = input_ids.to(device)

            # Generar la respuesta
            with torch.inference_mode():
                generated_ids = root.model.generate(
                    input_ids,
                    max_new_tokens=int(max_words.get()),
                    no_repeat_ngram_size=3,
                    do_sample=True,
                    top_k=40,
                    top_p=0.8,
                    temperature=0.5,
                    eos_token_id=root.tokenizer.eos_token_id,
                    pad_token_id=root.tokenizer.eos_token_id,
                    use_cache=True,  # Usar caché para mayor rendimiento
                )

            # Decodificar la respuesta
            response = root.tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)

            # Añadir la respuesta al historial
            root.conversation_history.append(f"<{bot_name.get()}>: {response}\n")

            # Mostrar la respuesta en la interfaz
            chat_log.config(state=tk.NORMAL)
            chat_log.insert(tk.END, f"{bot_name.get()}: {response}\n\n")
            chat_log.config(state=tk.DISABLED)
            chat_log.see(tk.END)
        except Exception as e:
            chat_log.config(state=tk.NORMAL)
            chat_log.insert(tk.END, f"Error al generar la respuesta: {e}\n\n")
            chat_log.config(state=tk.DISABLED)
            chat_log.see(tk.END)

    threading.Thread(target=run_model).start()

def clear_chat(event=None):
    chat_log.config(state=tk.NORMAL)
    chat_log.delete(1.0, tk.END)
    chat_log.insert(tk.END, "Chat borrado. Puedes comenzar una nueva conversación.\n\n")
    chat_log.config(state=tk.DISABLED)
    # Reiniciar el historial de conversación
    if hasattr(root, 'conversation_history'):
        del root.conversation_history

# Configuración de la interfaz gráfica
root = tk.Tk()
root.title("Chat by Viaja Tech")
root.configure(bg='black')
root.geometry('800x700')

# Área de chat con scroll
chat_log = scrolledtext.ScrolledText(root, state=tk.DISABLED, bg='black', fg='white', wrap=tk.WORD)
chat_log.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)

# Frame para opciones adicionales
options_frame = tk.Frame(root, bg='black')
options_frame.pack(padx=10, pady=5, fill=tk.X)

# Campo para ingresar el nombre del usuario
tk.Label(options_frame, text="Tu nombre:", bg='black', fg='white').pack(side=tk.LEFT)
user_name = tk.Entry(options_frame, width=15, bg='gray20', fg='white')
user_name.insert(0, "Usuario")  # Nombre por defecto
user_name.pack(side=tk.LEFT, padx=5)

# Campo para ingresar el nombre del bot
tk.Label(options_frame, text="Nombre del bot:", bg='black', fg='white').pack(side=tk.LEFT)
bot_name = tk.Entry(options_frame, width=15, bg='gray20', fg='white')
bot_name.insert(0, "Asistente")  # Nombre por defecto
bot_name.pack(side=tk.LEFT, padx=5)

# Campo para seleccionar el número máximo de palabras en la respuesta
tk.Label(options_frame, text="Máx. palabras:", bg='black', fg='white').pack(side=tk.LEFT)
max_words = tk.Entry(options_frame, width=5, bg='gray20', fg='white')
max_words.insert(0, "150")  # Valor por defecto
max_words.pack(side=tk.LEFT, padx=5)

# Botón para borrar el chat
clear_button = tk.Button(options_frame, text="Borrar Chat", command=clear_chat, bg='gray30', fg='white', state=tk.DISABLED)
clear_button.pack(side=tk.RIGHT, padx=5)

# Campo de entrada de usuario
user_entry = tk.Entry(root, width=100, bg='gray20', fg='white', state=tk.DISABLED)
user_entry.pack(padx=10, pady=(0, 10), fill=tk.X)
user_entry.bind("<Return>", generate_response)  # Enviar mensaje con Enter
root.bind("<Delete>", clear_chat)  # Borrar chat con Delete

# Botón enviar
send_button = tk.Button(root, text="Enviar", command=generate_response, bg='gray30', fg='white', state=tk.DISABLED)
send_button.pack(pady=(0, 10))

# Etiqueta de estado y barra de progreso
status_label = tk.Label(root, text="Iniciando...", bg='black', fg='white')
status_label.pack(pady=(0, 5))

progress_bar = ttk.Progressbar(root, mode='indeterminate')
progress_bar.pack(padx=10, fill=tk.X)

# Mensaje inicial o historia
chat_log.config(state=tk.NORMAL)
chat_log.insert(tk.END, "Bienvenido al Chat de Viaja Tech. Antes de empezar, te contaré una historia...\n\n")
chat_log.config(state=tk.DISABLED)

# Iniciar carga del modelo
load_model()

# Iniciar la interfaz gráfica
root.mainloop()
