#pip install torch torchvision torchaudio transformers accelerate bitsandbytes gradio



from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import gradio as gr
import torch

# Tu token de Hugging Face
HF_TOKEN = "tu_token_aquí"  # Reemplaza con tu token de Hugging Face

def load_model():
    """
    Carga el modelo LLaMA 3.2-3B con optimización de memoria en modo 8-bit.
    """
    model_name = "meta-llama/Llama-3.2-3B"
    print("Cargando el tokenizador...")

    # Cargar el tokenizador con autenticación
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        use_auth_token=HF_TOKEN
    )

    print("Cargando el modelo...")
    # Configuración de BitsAndBytes para optimización en 8-bit
    bnb_config = BitsAndBytesConfig(
        load_in_8bit=True,  # Carga el modelo en 8-bit para reducir uso de memoria
        quantization_config={"bnb_8bit": True}
    )

    # Cargar el modelo con autenticación
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto" if torch.cuda.is_available() else "cpu",
        use_auth_token=HF_TOKEN
    )

    print("Modelo cargado exitosamente.")
    return model, tokenizer

def generate_text(input_text, model, tokenizer):
    """
    Genera texto basado en el input proporcionado.
    """
    inputs = tokenizer(input_text, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
    outputs = model.generate(inputs["input_ids"], max_length=100, temperature=0.7, top_k=50)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

def main():
    """
    Configura el modelo y lanza la interfaz interactiva con Gradio.
    """
    print("Cargando el modelo...")
    model, tokenizer = load_model()

    def chat_interface(input_text):
        """
        Interfaz para interactuar con el modelo desde Gradio.
        """
        return generate_text(input_text, model, tokenizer)

    print("Iniciando la interfaz...")
    interface = gr.Interface(
        fn=chat_interface,
        inputs="text",
        outputs="text",
        title="Chat con LLaMA 3.2-3B",
        description="Interactúa con el modelo LLaMA 3.2-3B localmente."
    )
    interface.launch(share=True)

if __name__ == "__main__":
    main()
